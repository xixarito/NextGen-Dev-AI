{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea4eedc",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2 langchain tiktoken chromadb openai pytest pytest-mock pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e19bdb",
   "metadata": {},
   "source": [
    "## 1. Extracting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_loader.py\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae todo el texto de un archivo PDF.\n",
    "    Args:\n",
    "      pdf_path (str): Ruta al archivo PDF.\n",
    "    Returns:\n",
    "      str: Texto completo extraído.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = []\n",
    "    for page in reader.pages:\n",
    "        full_text.append(page.extract_text())\n",
    "    return \"\\n\".join(filter(None, full_text))\n",
    "\n",
    "# Test de unidad básico\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./test_docs/manual_empleado.pdf\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"Extracted text length: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83e77a",
   "metadata": {},
   "source": [
    "## 2. Text Splitting into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking.py\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks compatibles con LangChain.\n",
    "    \n",
    "    Args:\n",
    "      text (str): Texto completo a dividir.\n",
    "      chunk_size (int): Tamaño máximo por chunk.\n",
    "      chunk_overlap (int): Tamaño de overlap para mantener contexto.\n",
    "      \n",
    "    Returns:\n",
    "      List[str]: Lista de fragmentos de texto.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Test unidad básico\n",
    "if __name__ == \"__main__\":\n",
    "    chunks = chunk_text(text)\n",
    "    print(f\"Chunks created: {len(chunks)}\")\n",
    "    print(f\"First chunk preview: {chunks[0][:2000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06e27f",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.py\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "from torch import chunk\n",
    "\n",
    "def batch_texts_by_token_limit(texts: list[str], max_tokens: int = 300000, model_name: str = \"text-embedding-3-small\") -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide la lista de textos en lotes para no exceder el límite de tokens por solicitud.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    for text in texts:\n",
    "        tokens = len(enc.encode(text))\n",
    "        if current_tokens + tokens > max_tokens and current_batch:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "        current_batch.append(text)\n",
    "        current_tokens += tokens\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    return batches\n",
    "\n",
    "def create_openai_embeddings(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Crea embeddings usando el modelo text-embedding-3-small de OpenAI, respetando el límite de tokens por solicitud.\n",
    "    Args:\n",
    "      texts (list[str]): Lista de textos a vectorizar.\n",
    "      openai_api_key (str): API Key OpenAI.\n",
    "    Returns:\n",
    "      list[list[float]]: Vectores embedding.\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    batches = batch_texts_by_token_limit(texts)\n",
    "    vectors = []\n",
    "    for batch in batches:\n",
    "        vectors.extend(embedding_model.embed_documents(batch))\n",
    "    return vectors\n",
    "\n",
    "# Test básico\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings = create_openai_embeddings(chunks)\n",
    "    print(f\"Embedding vector length for first text: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c82e20",
   "metadata": {},
   "source": [
    "## 4. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d89880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.py\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "def save_embeddings_in_chromadb(\n",
    "    texts: list[str],\n",
    "    metadatas: list[dict],\n",
    "    persist_directory: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Guarda fragmentos y embeddings en ChromaDB para búsqueda.\n",
    "    Args:\n",
    "      texts: Lista de documentos/textos.\n",
    "      metadatas: Lista de diccionarios con metadatos, mismo orden que texts.\n",
    "      persist_directory (str): Carpeta donde persistir la DB.\n",
    "      openai_api_key (str): API Key OpenAI para crear embeddings.\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"internal_docs\",\n",
    "        embedding_function=embedding_model,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    vectorstore.persist()\n",
    "    vectorstore = None  # Liberar memoria\n",
    "\n",
    "    # Test básico\n",
    "if __name__ == \"__main__\":\n",
    "    sample_metadata = [{\"source\": pdf_path}]\n",
    "\n",
    "    batches = batch_texts_by_token_limit(chunks)\n",
    "    for batch in batches:\n",
    "        save_embeddings_in_chromadb(batch, sample_metadata, \"./db\")\n",
    "    print(\"Embeddings guardados con éxito.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da1ee7",
   "metadata": {},
   "source": [
    "## 5. Retriving from the Persistant Vector Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.py\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "def load_retriever(persist_directory: str, openai_api_key: str):\n",
    "    \"\"\"\n",
    "    Carga la base ChromaDB como vectorstore para realizar consultas.\n",
    "    Retorna el objeto retriever.\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"internal_docs\",\n",
    "        embedding_function=embedding_model,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "    return retriever\n",
    "\n",
    "# Test básico: recuperar textos similares\n",
    "if __name__ == \"__main__\":\n",
    "    retriever = load_retriever(\"./db\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "    query = \"¿Cuáles son las políticas de seguridad?\"\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    print(f\"Documentos recuperados: {len(docs)}\")\n",
    "    print(f\"Primera doc preview: {docs[0].page_content[:300] if docs else 'Sin resultados'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abf77e",
   "metadata": {},
   "source": [
    "## 6. Retrivers in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_answer.py\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "def generate_answer_from_rag(question: str, retriever, openai_api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Dada una pregunta y un retriever, genera una respuesta usando RAG y modelo GPT.\n",
    "    \n",
    "    Args:\n",
    "      question (str): Pregunta a responder.\n",
    "      retriever: Retriever previamente inicializado (ChromaDB).\n",
    "      openai_api_key (str): API key para OpenAI.\n",
    "      \n",
    "    Returns:\n",
    "      str: Respuesta generada.\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    result = qa_chain.run(question)\n",
    "    return result\n",
    "\n",
    "# Test básico\n",
    "if __name__ == \"__main__\":\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    retriever = load_retriever(\"./db\", openai_api_key)\n",
    "    question = \"¿Qué procedimientos debo seguir en caso de emergencia laboral?\"\n",
    "    answer = generate_answer_from_rag(question, retriever, openai_api_key)\n",
    "    print(f\"Respuesta:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b29c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_rag_system.py\n",
    "import pytest\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def setup_teardown():\n",
    "    \"\"\"\n",
    "    Fixture para preparar entorno con embedding y textos de ejemplo y limpiar después.\n",
    "    \"\"\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if openai_api_key is None:\n",
    "        pytest.skip(\"OPENAI_API_KEY no está configurado en el entorno de prueba.\")\n",
    "    \n",
    "    # Paso 1: Extraer texto dummy (se podría usar pdf real aquí)\n",
    "    sample_text = \"Las políticas de seguridad establecen que \" \\\n",
    "                  \"todo empleado debe reportar incidentes.\\n\" \\\n",
    "                  \"En caso de emergencia laboral, sigue el protocolo ABC.\\n\" \\\n",
    "                  \"El horario de trabajo es de 9 a 18 hrs con pausa para almuerzo.\\n\" \\\n",
    "                  \"Los reportes trimestrales están basados en indicadores XYZ.\\n\" \\\n",
    "                  \"Cualquier cambio en política debe ser aprobado por RRHH.\"\n",
    "    chunks = chunk_text(sample_text)\n",
    "    metadatas = [{\"source\": f\"chunk-{i}\"} for i in range(len(chunks))]\n",
    "    \n",
    "    persist_dir = \"./test_db\"\n",
    "    \n",
    "    # Guardar embeddings\n",
    "    save_embeddings_in_chromadb(chunks, metadatas, persist_dir, openai_api_key)\n",
    "    \n",
    "    yield openai_api_key, persist_dir\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(persist_dir):\n",
    "        shutil.rmtree(persist_dir)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"question\", [\n",
    "    \"¿Cuáles son las políticas de seguridad?\",\n",
    "    \"¿Qué hacer en caso de emergencia laboral?\",\n",
    "    \"¿Cuál es el horario de trabajo?\",\n",
    "    \"¿En qué se basan los reportes trimestrales?\",\n",
    "    \"¿Quién aprueba los cambios en las políticas?\"\n",
    "])\n",
    "def test_rag_answers(setup_teardown, question):\n",
    "    openai_api_key, persist_dir = setup_teardown\n",
    "    retriever = load_retriever(persist_dir, openai_api_key)\n",
    "    answer = generate_answer_from_rag(question, retriever, openai_api_key)\n",
    "    \n",
    "    assert isinstance(answer, str)\n",
    "    assert len(answer.strip()) > 0\n",
    "    # Extra simple: la respuesta debe tener al menos 10 caracteres.\n",
    "    assert len(answer) > 10\n",
    "\n",
    "\n",
    "# Seguridad básica: test contra inyección SQL simulada (en este contexto vectors, es más prevención de inputs malignos)\n",
    "def test_sql_injection_prevention(setup_teardown):\n",
    "    openai_api_key, persist_dir = setup_teardown\n",
    "    retriever = load_retriever(persist_dir, openai_api_key)\n",
    "    injection_string = \"'; DROP TABLE users; --\"\n",
    "    answer = generate_answer_from_rag(injection_string, retriever, openai_api_key)\n",
    "    # Validamos que el código no explota y responde algo\n",
    "    assert answer is not None\n",
    "\n",
    "\n",
    "# Contract test: Validar esquema Response OpenAPI simple (mock de esquema)\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "class OpenAPIResponseSchema(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "def test_openapi_contract(setup_teardown):\n",
    "    openai_api_key, persist_dir = setup_teardown\n",
    "    retriever = load_retriever(persist_dir, openai_api_key)\n",
    "    question = \"¿Cuál es el protocolo de emergencias?\"\n",
    "    answer = generate_answer_from_rag(question, retriever, openai_api_key)\n",
    "    try:\n",
    "        # Simular que la respuesta viene en formato JSON { \"answer\": <respuesta> }\n",
    "        data = {\"answer\": answer}\n",
    "        validated = OpenAPIResponseSchema(**data)\n",
    "        assert validated.answer == answer\n",
    "    except ValidationError:\n",
    "        pytest.fail(\"Respuesta no cumple esquema OpenAPI esperado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
